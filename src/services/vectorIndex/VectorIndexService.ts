import { randomUUID } from 'crypto';
import * as path from 'path';
import { s3Service } from '../s3Service';
import { contentExtractionService } from '../contentExtractionService';
import { lambdaIndexerService } from '../lambdaIndexer';
import { EmbeddingService } from './EmbeddingService';
import { HNSWIndexManager } from './HNSWIndexManager';
import { FaissIndexManager } from './FaissIndexManager';
import { MetadataStore } from './MetadataStore';
import { ProgressTracker } from './ProgressTracker';
import { CheckpointManager } from './CheckpointManager';
import { JobLockManager } from './JobLockManager';
import {
  VectorIndexJob,
  VectorIndexStatus,
  VectorIndexStage,
  VectorIndexConfig,
  VectorIndexManifest,
  VectorMetadata,
  VectorSearchRequest,
  VectorSearchResult,
} from '../../types/vectorIndex';
import { S3Client, PutObjectCommand, GetObjectCommand, ListObjectsV2Command } from '@aws-sdk/client-s3';
import * as fs from 'fs';
import * as crypto from 'crypto';

const s3Client = new S3Client({ region: process.env.AWS_REGION || 'us-east-1' });
const s3BucketName = process.env.S3_BUCKET_NAME || '';

const DEFAULT_CONFIG: VectorIndexConfig = {
  batchSize: 100,
  maxChunksInMemory: 100,
  maxVectorsInMemory: 500,
  checkpointInterval: 50,
  gcInterval: 200,
  maxRetries: 3,
  embeddingModel: 'text-embedding-3-small',
  dimensions: 1536,
  indexType: 'hnsw',
};

export class VectorIndexService {
  private jobs: Map<string, VectorIndexJob> = new Map();
  private config: VectorIndexConfig;
  private embeddingService: EmbeddingService;
  private jobLockManager: JobLockManager;
  private checkpointManager: CheckpointManager;
  private tempDir: string = './data/temp';
  private jobsLoaded: boolean = false;
  private readonly JOB_RETENTION_DAYS = 7;

  constructor(config: Partial<VectorIndexConfig> = {}) {
    this.config = { ...DEFAULT_CONFIG, ...config };
    this.embeddingService = new EmbeddingService(
      this.config.embeddingModel,
      this.config.dimensions,
      this.config.maxRetries
    );
    this.jobLockManager = new JobLockManager();
    this.checkpointManager = new CheckpointManager();

    if (!fs.existsSync(this.tempDir)) {
      fs.mkdirSync(this.tempDir, { recursive: true });
    }

    // å•Ÿå‹•æ™‚å¾ S3 è¼‰å…¥ jobs
    this.loadJobsFromS3().catch(error => {
      console.error('Failed to load jobs from S3:', error);
    });

    // æ¯å¤©æ¸…ç†ä¸€æ¬¡éæœŸçš„ jobs
    setInterval(() => {
      this.cleanupOldJobsFromS3().catch(error => {
        console.error('Failed to cleanup old jobs:', error);
      });
    }, 24 * 60 * 60 * 1000); // 24 å°æ™‚
  }

  /**
   * å•Ÿå‹•å‘é‡ç´¢å¼•ä»»å‹™
   */
  async startIndexing(fileId: string, fileName: string, forceRebuild: boolean = false): Promise<string> {
    // å…ˆæ¸…ç†éæœŸçš„é–å®š
    this.jobLockManager.cleanupExpiredLocks();

    // æª¢æŸ¥å…¨åŸŸé–å®š
    if (this.jobLockManager.isGloballyLocked()) {
      if (!forceRebuild) {
        throw new Error('Another indexing job is in progress. Please wait.');
      }
      console.warn('Force rebuild requested, releasing existing lock');
      this.jobLockManager.releaseGlobalLock();
    }

    // æª¢æŸ¥æª”æ¡ˆé–å®š
    if (this.jobLockManager.isFileLocked(fileId)) {
      if (!forceRebuild) {
        throw new Error(`File ${fileId} is already being indexed.`);
      }
      console.warn(`Force rebuild requested for file ${fileId}, releasing existing lock`);
      this.jobLockManager.releaseFileLock(fileId);
    }

    const jobId = randomUUID();

    // å–å¾—å…¨åŸŸé–å®š
    if (!this.jobLockManager.acquireGlobalLock(jobId)) {
      throw new Error('Failed to acquire global lock');
    }

    // å–å¾—æª”æ¡ˆé–å®š
    if (!this.jobLockManager.acquireFileLock(fileId, jobId)) {
      this.jobLockManager.releaseGlobalLock();
      throw new Error(`Failed to acquire lock for file ${fileId}`);
    }

    // å»ºç«‹ä»»å‹™
    const job: VectorIndexJob = {
      jobId,
      fileId,
      fileName,
      status: VectorIndexStatus.PENDING,
      stage: VectorIndexStage.INITIALIZATION,
      progress: {
        current: 0,
        total: 0,
        percentage: 0,
      },
      createdAt: new Date(),
      updatedAt: new Date(),
    };

    this.jobs.set(jobId, job);

    // ä¿å­˜ job åˆ° S3
    await this.saveJobToS3(job);

    // åœ¨èƒŒæ™¯åŸ·è¡Œç´¢å¼•
    this.processIndexing(jobId, fileId, fileName).catch(error => {
      console.error(`Indexing job ${jobId} failed:`, error);
      this.handleJobError(jobId, error);
      // ç¢ºä¿é‡‹æ”¾é–å®š
      this.cleanup(jobId, fileId);
    });

    return jobId;
  }

  /**
   * è™•ç†ç´¢å¼•æµç¨‹ (ä½¿ç”¨ Lambda è™•ç†)
   */
  private async processIndexing(jobId: string, fileId: string, fileName: string): Promise<void> {
    const job = this.jobs.get(jobId);
    if (!job) {
      console.error(`Job ${jobId} not found`);
      return;
    }

    try {
      // æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚ºé€²è¡Œä¸­
      job.status = VectorIndexStatus.EXTRACTING;
      job.stage = VectorIndexStage.TEXT_EXTRACTION;
      job.updatedAt = new Date();

      // ç²å–æª”æ¡ˆçš„ S3 Key
      const files = await s3Service.listFiles();
      const fileMetadata = files.find(f => f.id === fileId);
      
      if (!fileMetadata) {
        throw new Error(`File metadata not found for ${fileId}`);
      }

      console.log(`ğŸš€ Triggering Lambda indexer for ${fileName} (${fileId})`);
      console.log(`   S3 Key: ${fileMetadata.s3Key}`);

      // è§¸ç™¼ Lambda ç´¢å¼•
      const success = await lambdaIndexerService.triggerIndexing(
        fileId,
        fileName,
        fileMetadata.s3Key,
        jobId
      );

      if (success) {
        // Lambda å·²æˆåŠŸè§¸ç™¼ï¼Œæ›´æ–°ä»»å‹™ç‹€æ…‹
        job.status = VectorIndexStatus.INDEXING;
        job.stage = VectorIndexStage.INDEX_BUILDING;
        job.updatedAt = new Date();
        
        console.log(`âœ… Lambda indexer triggered successfully for ${fileName}`);
        console.log(`   Job ID: ${jobId}`);
        console.log(`   Lambda will callback when indexing completes`);
      } else {
        throw new Error('Failed to trigger Lambda indexer');
      }
    } catch (error: any) {
      console.error(`âŒ Lambda indexing failed for ${fileName}:`, error.message);
      
      if (job) {
        job.status = VectorIndexStatus.FAILED;
        job.error = error.message;
        job.updatedAt = new Date();
      }
    } finally {
      // æ³¨æ„ï¼šä¸è¦åœ¨é€™è£¡é‡‹æ”¾é–å®š
      // Lambda callback æœƒè² è²¬å®Œæˆå¾Œçš„æ¸…ç†å·¥ä½œ
    }
  }

  /**
   * å¾é€²åº¦è¿½è¹¤å™¨æ›´æ–°ä»»å‹™
   */
  private updateJobFromTracker(jobId: string, tracker: ProgressTracker): void {
    const job = this.jobs.get(jobId);
    if (job) {
      const stats = tracker.getStats();
      job.status = stats.currentStatus;
      job.stage = stats.currentStage;
      job.progress = stats.progress;
      job.updatedAt = new Date();
    }
  }

  /**
   * è™•ç†ä»»å‹™éŒ¯èª¤
   */
  private handleJobError(jobId: string, error: any): void {
    const job = this.jobs.get(jobId);
    if (job) {
      job.status = VectorIndexStatus.FAILED;
      job.error = error.message || String(error);
      job.updatedAt = new Date();
      
      // é‡‹æ”¾é–å®š
      if (job.fileId) {
        this.jobLockManager.releaseFileLock(job.fileId);
      }
      this.jobLockManager.releaseGlobalLock();
    }
  }

  /**
   * ä¸Šå‚³æª”æ¡ˆåˆ° S3
   */
  private async uploadFileToS3(localPath: string, s3Key: string): Promise<string> {
    const fileContent = fs.readFileSync(localPath);
    
    await s3Client.send(new PutObjectCommand({
      Bucket: s3BucketName,
      Key: s3Key,
      Body: fileContent,
    }));

    return `s3://${s3BucketName}/${s3Key}`;
  }

  /**
   * è¨ˆç®—æª”æ¡ˆ checksum
   */
  private async calculateChecksum(filePath: string): Promise<string> {
    const content = fs.readFileSync(filePath);
    return crypto.createHash('sha256').update(content).digest('hex');
  }

  /**
   * æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
   */
  private cleanup(jobId: string, fileId: string, ...filePaths: string[]): void {
    // åˆªé™¤è‡¨æ™‚æª”æ¡ˆ
    for (const filePath of filePaths) {
      if (fs.existsSync(filePath)) {
        fs.unlinkSync(filePath);
      }
    }

    // é‡‹æ”¾é–å®š
    this.jobLockManager.releaseFileLock(fileId);
    this.jobLockManager.releaseGlobalLock();

    console.log(`Cleanup completed for job ${jobId}`);
  }

  /**
   * å–å¾—ä»»å‹™ç‹€æ…‹
   */
  getJob(jobId: string): VectorIndexJob | undefined {
    const job = this.jobs.get(jobId);
    if (!job) return undefined;

    const now = Date.now();
    const timeout = 15 * 60 * 1000; // 15 åˆ†é˜è¶…æ™‚

    // æª¢æŸ¥æ˜¯å¦è¶…æ™‚ï¼ˆåªæª¢æŸ¥æœªå®Œæˆçš„ä»»å‹™ï¼‰
    if (
      !job.completedAt && // æ²’æœ‰å®Œæˆæ™‚é–“
      (job.status === VectorIndexStatus.INDEXING || 
       job.status === VectorIndexStatus.EXTRACTING ||
       job.status === VectorIndexStatus.PENDING) &&
      now - job.createdAt.getTime() > timeout
    ) {
      console.warn(`Job ${job.jobId} has timed out after ${timeout / 1000}s`);
      job.status = VectorIndexStatus.FAILED;
      job.error = `Timeout: No callback received after ${timeout / 60000} minutes. Lambda may have failed silently.`;
      job.updatedAt = new Date();
      
      // é‡‹æ”¾é–å®š
      this.jobLockManager.releaseFileLock(job.fileId);
      this.jobLockManager.releaseGlobalLock();

      // ä¿å­˜æ›´æ–°åˆ° S3
      this.saveJobToS3(job).catch(err => {
        console.error(`Failed to save timeout update for job ${job.jobId}:`, err);
      });
    }

    return job;
  }

  /**
   * å¾ Lambda callback æ›´æ–°ä»»å‹™ç‹€æ…‹
   */
  async updateJobFromCallback(
    jobId: string, 
    success: boolean, 
    error?: string,
    metrics?: {
      processingTime?: number;
      stats?: any;
      numChunks?: number;
      numVectors?: number;
    }
  ): Promise<void> {
    const job = this.jobs.get(jobId);
    if (!job) {
      console.warn(`Job ${jobId} not found for callback update`);
      return;
    }

    try {
      if (success) {
        job.status = VectorIndexStatus.COMPLETED;
        job.stage = VectorIndexStage.COMPLETED;
        job.progress = {
          current: 100,
          total: 100,
          percentage: 100,
        };
        job.completedAt = new Date();
        job.updatedAt = new Date();

        // æ›´æ–°çµ±è¨ˆè³‡æ–™
        if (metrics) {
          job.processingTime = metrics.processingTime;
          job.stats = metrics.stats;
          job.numChunks = metrics.numChunks;
          job.numVectors = metrics.numVectors;

          // è¨ˆç®—è²»ç”¨
          job.costs = this.calculateCosts(metrics);
        }
        
        console.log(`âœ… Job ${jobId} completed successfully via Lambda callback`);
        if (job.processingTime) {
          console.log(`   Processing time: ${job.processingTime.toFixed(2)}s`);
        }
        if (job.costs) {
          console.log(`   Estimated cost: $${job.costs.total.toFixed(6)}`);
        }
      } else {
        job.status = VectorIndexStatus.FAILED;
        job.error = error || 'Lambda indexing failed';
        job.updatedAt = new Date();
        
        console.error(`âŒ Job ${jobId} failed via Lambda callback: ${error}`);
      }

      // ä¿å­˜æ›´æ–°åˆ° S3
      await this.saveJobToS3(job);
    } finally {
      // é‡‹æ”¾é–å®š
      this.jobLockManager.releaseFileLock(job.fileId);
      this.jobLockManager.releaseGlobalLock();
      
      console.log(`ğŸ”“ Locks released for job ${jobId}`);
    }
  }

  /**
   * è¨ˆç®—åŸ·è¡Œè²»ç”¨
   */
  private calculateCosts(metrics: {
    processingTime?: number;
    stats?: any;
    numChunks?: number;
    numVectors?: number;
  }): { lambda: number; embedding: number; total: number } {
    const costs = {
      lambda: 0,
      embedding: 0,
      total: 0
    };

    // Lambda è²»ç”¨è¨ˆç®—
    // AWS Lambda å®šåƒ¹ (us-east-1, 2024):
    // - $0.0000166667 per GB-second (128MB = 0.125GB)
    // - $0.20 per 1M requests
    if (metrics.processingTime) {
      const memoryGB = 0.512; // 512MB = 0.5GB
      const gbSeconds = memoryGB * metrics.processingTime;
      costs.lambda = gbSeconds * 0.0000166667 + 0.0000002; // + per request cost
    }

    // OpenAI Embedding API è²»ç”¨
    // text-embedding-3-small: $0.00002 per 1K tokens
    // å‡è¨­å¹³å‡æ¯å€‹ chunk ç´„ 500 tokens
    if (metrics.numVectors) {
      const estimatedTokens = metrics.numVectors * 500;
      costs.embedding = (estimatedTokens / 1000) * 0.00002;
    }

    costs.total = costs.lambda + costs.embedding;

    return costs;
  }

  /**
   * åˆ—å‡ºæ‰€æœ‰ä»»å‹™
   */
  listJobs(): VectorIndexJob[] {
    const jobs = Array.from(this.jobs.values());
    const now = Date.now();
    const timeout = 15 * 60 * 1000; // 15 åˆ†é˜è¶…æ™‚

    // æª¢æŸ¥ä¸¦æ¨™è¨˜è¶…æ™‚çš„ä»»å‹™ï¼ˆåªæª¢æŸ¥æœªå®Œæˆçš„ä»»å‹™ï¼‰
    for (const job of jobs) {
      if (
        !job.completedAt && // æ²’æœ‰å®Œæˆæ™‚é–“
        (job.status === VectorIndexStatus.INDEXING || 
         job.status === VectorIndexStatus.EXTRACTING ||
         job.status === VectorIndexStatus.PENDING) &&
        now - job.createdAt.getTime() > timeout
      ) {
        console.warn(`Job ${job.jobId} has timed out after ${timeout / 1000}s`);
        job.status = VectorIndexStatus.FAILED;
        job.error = `Timeout: No callback received after ${timeout / 60000} minutes. Lambda may have failed silently.`;
        job.updatedAt = new Date();
        
        // é‡‹æ”¾é–å®š
        this.jobLockManager.releaseFileLock(job.fileId);
        this.jobLockManager.releaseGlobalLock();

        // ä¿å­˜æ›´æ–°åˆ° S3
        this.saveJobToS3(job).catch(err => {
          console.error(`Failed to save timeout update for job ${job.jobId}:`, err);
        });
      }
    }

    return jobs;
  }

  /**
   * å–æ¶ˆä»»å‹™
   */
  async cancelJob(jobId: string): Promise<boolean> {
    const job = this.jobs.get(jobId);
    if (!job) {
      return false;
    }

    if (job.status === VectorIndexStatus.COMPLETED || job.status === VectorIndexStatus.FAILED) {
      return false;
    }

    job.status = VectorIndexStatus.CANCELLED;
    job.updatedAt = new Date();
    
    this.jobLockManager.releaseFileLock(job.fileId);
    this.jobLockManager.releaseGlobalLock();

    // ä¿å­˜æ›´æ–°åˆ° S3
    await this.saveJobToS3(job);

    return true;
  }

  /**
   * æœç´¢å‘é‡ç´¢å¼•
   */
  async searchVector(request: VectorSearchRequest): Promise<VectorSearchResult[]> {
    const { fileId, query, k = 5, minScore = 0.0 } = request;

    if (!fileId) {
      throw new Error('fileId is required for vector search');
    }

    // 1. å¾ S3 ä¸‹è¼‰ç´¢å¼•å’Œå…ƒæ•¸æ“š
    const indexKey = `vector-indexes/${fileId}.index`;
    const metadataKey = `vector-indexes/${fileId}.metadata.json`;

    let indexBuffer: Buffer;
    let rawMetadata: any[];

    try {
      // ä¸‹è¼‰ç´¢å¼•æª”æ¡ˆ
      const indexCmd = new GetObjectCommand({
        Bucket: s3BucketName,
        Key: indexKey,
      });
      const indexResponse = await s3Client.send(indexCmd);
      indexBuffer = await this.streamToBuffer(indexResponse.Body);

      // ä¸‹è¼‰å…ƒæ•¸æ“š
      const metadataCmd = new GetObjectCommand({
        Bucket: s3BucketName,
        Key: metadataKey,
      });
      const metadataResponse = await s3Client.send(metadataCmd);
      const metadataBuffer = await this.streamToBuffer(metadataResponse.Body);
      rawMetadata = JSON.parse(metadataBuffer.toString('utf-8'));
    } catch (error) {
      throw new Error(
        `Failed to load vector index for file ${fileId}: ${
          error instanceof Error ? error.message : String(error)
        }`
      );
    }

    // 2. å„²å­˜ç´¢å¼•åˆ°è‡¨æ™‚æª”æ¡ˆä¸¦è¼‰å…¥
    const tempIndexPath = path.join(this.tempDir, `search-${fileId}-${Date.now()}.index`);
    fs.writeFileSync(tempIndexPath, indexBuffer);

    const indexManager = new FaissIndexManager(this.config.dimensions);

    try {
      await indexManager.load(tempIndexPath);

      // 3. å°‡æŸ¥è©¢æ–‡å­—è½‰ç‚ºå‘é‡
      const queryEmbedding = await this.embeddingService.embedText(query);

      // 4. æœç´¢æœ€è¿‘é„° (FAISS uses Inner Product, higher is better)
      const searchResults = await indexManager.search(queryEmbedding, k);

      // 5. çµ„åˆçµæœ - ä½¿ç”¨å¯¦éš›çš„ metadata æ¬„ä½åç¨±
      const results: VectorSearchResult[] = [];
      for (let i = 0; i < searchResults.labels.length; i++) {
        const vectorId = searchResults.labels[i];
        const distance = searchResults.distances[i];
        
        // FAISS Inner Product: higher score = more similar
        const score = distance;

        if (score < minScore) {
          continue;
        }

        const rawMeta = rawMetadata[vectorId];
        if (rawMeta) {
          // å°‡åŸå§‹ metadata è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
          const standardMetadata: VectorMetadata = {
            chunkId: `${rawMeta.fileId}-chunk-${rawMeta.chunkIndex}`,
            fileId: rawMeta.fileId,
            vectorId: vectorId,
            content: rawMeta.text || '',
            startIndex: rawMeta.startWord || 0,
            endIndex: rawMeta.endWord || 0,
            chunkOrder: rawMeta.chunkIndex || 0,
            createdAt: new Date(),
          };

          results.push({
            chunkId: standardMetadata.chunkId,
            fileId: standardMetadata.fileId,
            content: standardMetadata.content,
            score,
            metadata: standardMetadata,
          });
        }
      }

      return results;
    } finally {
      // æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
      if (fs.existsSync(tempIndexPath)) {
        fs.unlinkSync(tempIndexPath);
      }
    }
  }

  /**
   * è¼”åŠ©å‡½æ•¸ï¼šå°‡ Stream è½‰ç‚º Buffer
   */
  private async streamToBuffer(stream: any): Promise<Buffer> {
    const chunks: Buffer[] = [];
    for await (const chunk of stream) {
      chunks.push(chunk);
    }
    return Buffer.concat(chunks);
  }

  /**
   * Search across all indexed manuals simultaneously
   */
  async searchAllManuals(query: string, k: number = 5, minScore: number = 0.5): Promise<VectorSearchResult[]> {
    try {
      const listCmd = new ListObjectsV2Command({
        Bucket: s3BucketName,
        Prefix: 'vector-indexes/',
      });
      
      const response = await s3Client.send(listCmd);
      const indexFiles = (response.Contents || [])
        .filter((obj) => obj.Key?.endsWith('.index'))
        .map((obj) => {
          const match = obj.Key?.match(/vector-indexes\/(.+)\.index$/);
          return match ? match[1] : null;
        })
        .filter((id): id is string => id !== null);

      if (indexFiles.length === 0) {
        return [];
      }

      // Search each file and collect results
      const allResults: VectorSearchResult[] = [];
      
      for (const fileId of indexFiles) {
        try {
          const results = await this.searchVector({
            fileId,
            query,
            k: Math.ceil(k / indexFiles.length) + 2,
            minScore,
          });
          allResults.push(...results);
        } catch (error) {
          console.error(`Failed to search file ${fileId}:`, error);
        }
      }

      // Sort by score and return top k
      allResults.sort((a, b) => b.score - a.score);
      return allResults.slice(0, k);
      
    } catch (error) {
      throw new Error(
        `Failed to search all manuals: ${error instanceof Error ? error.message : String(error)}`
      );
    }
  }

  /**
   * å¾ S3 è¼‰å…¥æ‰€æœ‰ jobs
   */
  private async loadJobsFromS3(): Promise<void> {
    if (this.jobsLoaded) return;

    try {
      if (process.env.DEBUG_MCP) {
        console.error('Loading jobs from S3...');
      }
      const { ListObjectsV2Command } = await import('@aws-sdk/client-s3');
      
      const listCommand = new ListObjectsV2Command({
        Bucket: s3BucketName,
        Prefix: 'jobs/',
      });

      const listResponse = await s3Client.send(listCommand);
      
      if (!listResponse.Contents || listResponse.Contents.length === 0) {
        console.log('No jobs found in S3');
        this.jobsLoaded = true;
        return;
      }

      const cutoffDate = new Date();
      cutoffDate.setDate(cutoffDate.getDate() - this.JOB_RETENTION_DAYS);

      let loadedCount = 0;
      let skippedCount = 0;

      for (const item of listResponse.Contents) {
        if (!item.Key || !item.Key.endsWith('.json')) continue;

        // æª¢æŸ¥æ˜¯å¦éæœŸ
        if (item.LastModified && item.LastModified < cutoffDate) {
          skippedCount++;
          continue;
        }

        try {
          const getCommand = new GetObjectCommand({
            Bucket: s3BucketName,
            Key: item.Key,
          });

          const response = await s3Client.send(getCommand);
          const jobData = await this.streamToBuffer(response.Body);
          const job: VectorIndexJob = JSON.parse(jobData.toString('utf-8'));

          // è½‰æ›æ—¥æœŸå­—ä¸²ç‚º Date ç‰©ä»¶
          job.createdAt = new Date(job.createdAt);
          job.updatedAt = new Date(job.updatedAt);
          if (job.completedAt) {
            job.completedAt = new Date(job.completedAt);
          }

          this.jobs.set(job.jobId, job);
          loadedCount++;
        } catch (error) {
          console.error(`Failed to load job from ${item.Key}:`, error);
        }
      }

      console.log(`âœ“ Loaded ${loadedCount} jobs from S3 (skipped ${skippedCount} old jobs)`);
      this.jobsLoaded = true;
    } catch (error) {
      console.error('Error loading jobs from S3:', error);
      this.jobsLoaded = true;
    }
  }

  /**
   * ä¿å­˜å–®å€‹ job åˆ° S3
   */
  private async saveJobToS3(job: VectorIndexJob): Promise<void> {
    try {
      const jobKey = `jobs/${job.jobId}.json`;
      const jobData = JSON.stringify(job, null, 2);

      await s3Client.send(new PutObjectCommand({
        Bucket: s3BucketName,
        Key: jobKey,
        Body: jobData,
        ContentType: 'application/json',
      }));

      // console.log(`âœ“ Saved job ${job.jobId} to S3`);
    } catch (error) {
      console.error(`Failed to save job ${job.jobId} to S3:`, error);
    }
  }

  /**
   * å¾ S3 åˆªé™¤éæœŸçš„ jobs
   */
  private async cleanupOldJobsFromS3(): Promise<void> {
    try {
      const { ListObjectsV2Command, DeleteObjectCommand } = await import('@aws-sdk/client-s3');
      
      const listCommand = new ListObjectsV2Command({
        Bucket: s3BucketName,
        Prefix: 'jobs/',
      });

      const listResponse = await s3Client.send(listCommand);
      
      if (!listResponse.Contents) return;

      const cutoffDate = new Date();
      cutoffDate.setDate(cutoffDate.getDate() - this.JOB_RETENTION_DAYS);

      let deletedCount = 0;

      for (const item of listResponse.Contents) {
        if (!item.Key || !item.LastModified) continue;

        if (item.LastModified < cutoffDate) {
          try {
            await s3Client.send(new DeleteObjectCommand({
              Bucket: s3BucketName,
              Key: item.Key,
            }));
            deletedCount++;
          } catch (error) {
            console.error(`Failed to delete old job ${item.Key}:`, error);
          }
        }
      }

      if (deletedCount > 0) {
        console.log(`âœ“ Cleaned up ${deletedCount} old jobs from S3`);
      }
    } catch (error) {
      console.error('Error cleaning up old jobs from S3:', error);
    }
  }
}


// å–®ä¾‹
export const vectorIndexService = new VectorIndexService();
